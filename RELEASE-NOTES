version 0.10.0
--------------

- implement UTXO cache windowing - issue #71.  This should be a
  performance improvement both during initial sync, and also for
  processing new blocks when serving.

Please backup your DB or wait for stability to be confirmed before you
run this version.  It changes the DB flush logic and if there is an
error in my logic unrecoverable DB corruption is very likely.

version 0.9.9
-------------

- prioritize mempool processing of sent txs.  Closes issue 73.
- mempool tx processing needs to handle DBError exceptions.  Fixes issue 74.

version 0.9.8
-------------

- cleanup up mempool handling, notify of addresses only once when a new block
  comes in.  Fixes issue 70.

version 0.9.7
-------------

- history and UTXO requests are now processed by the executor, i.e.,
  properly asynchronously.  This was the last of the potential latency
  bottlenecks.

version 0.9.6
-------------

- fix it properly this time

version 0.9.5
-------------

- fix issue introduced in 0.9.4 with paused connections

version 0.9.4
-------------

- new env var MAX_SESSIONS, see docs/ENV-NOTES.  The default limit is
  1,000 sessions so raise this if you want to be able to take more.
- a couple of minor bug fixes relating to paused connections
- removed RPC calls numsessions and numpeers.  They're not very interesting
  and all that and more is in getinfo.

version 0.9.3
-------------

- unconfirmed flag indicating whether mempool txs have unconfirmed inputs
  was inverted

version 0.9.2
-------------

- fix mempool busy waiting

version 0.9.1
-------------

- fix another couple of issues introduced in 0.9.0

version 0.9.0a
--------------

- fix typo in 0.9.0

version 0.9.0
-------------

- complete rewrite of mempool code to have minimal latency and fix a
  couple of minor bugs.  When a new block is found, ideally this
  should be communicated to clients who addresses are affected with a
  single notification.  Previously this would happen with two
  notifications: one because the TX got in the block, and one because
  that TX was no longer in the mempool.  Fundamentally this a race
  condition that cannot be eliminated but its occurrence should be
  minimized.

version 0.8.12
--------------

- pause serving sessions whose send buffers are full (anti-DoS).  This
  is currently logged; let me know if it's too verbose
- various tweaks to request handling

version 0.8.11
--------------

- catch harmless socket exception
- show session count in groups RPC call

version 0.8.10
--------------

- fix socket bug in 0.8.9

version 0.8.9
-------------

- RPC groups and sessions calls improved
- issues fixed: #62, #68  (slow socket closing, IRC)

version 0.8.8
-------------

- put sessions in a priority queue to better prioritise serving.  Low-bandwidth
  sessions get served first
- new RPC command "groups" - shows information about session groups
- sessions output: session priority shown under Flags column; the lower the
  number the higher the priority.  txs column moved, new column reqs showns
  the number of outstanding requests for that connection (includes subrequests
  of batches)
- issued fixed: #67

version 0.8.7
-------------

- update systemd config (bauerj)
- temporary fix for initial sync times
- continued JSON code refactoring

version 0.8.6
-------------

- fix JSON bugs from 0.8.5
- fix issue #61 (valesi)

version 0.8.5
-------------

- rework of JSON layer to better handle batch requests.  This is
  preparatory work for improved DoS resistance.

I'm aware recent versions don't sync efficiently; please use 0.8.0 to sync
until I find time to fix it.

version 0.8.4
-------------

- remove invalidated histories from cache on new block

version 0.8.3
-------------

Minor tweaks to session logs:

- sessions output now shows flags.  All sessions are listed.  The
  session type column is gone, instead the first letter of RPC, SSL or
  TCP is the first flag letter.  A 'C' flag indicates the session is closing.
  An 'L' that it's being logged.
- don't attempt to forcefully stale sockets; they remain in C state until
  Python closes them (which can be a long time for some SSL sockets)
- don't consider all seeing eye connections as stale

version 0.8.2
-------------

- process new blocks in the asyncio executor; essentially a python thread.
  This should eliminate latency during block processing that caused sessions
  to be dropped.
- bandwith limit is restored incrementally to a session over the hour
  rather than in a lump when one hour has passed.  Also, only the
  limit is refunded each hour; previously the entire usage would be
  refunded.  So if the session uses 30MB bandwidth and your limit is
  10MB, it will take 3 hrs before the session is considered to have
  used none of its allotted bandwidth; previously it would happen after 1
  hour.

version 0.8.1
-------------

** NOTE: this version has a new Python package dependency: pylru

- fix for IRC encoding killing IRC connection
- add lru cache for history

version 0.8.0
------------

- stale connections are periodically closed.  See docs/ENV-NOTES for
  SESSION_TIMEOUT, default is 10 minutes.  Issue #56.
- each session gets its own ID which is used in the logs instead of its
  network address; the network address is still shown on initial connection.
  Issue #55.
- the session ID is also shown in the sessions list.  You can use this ID
  with the following new RPC commands which take a list of session ids:

     electrumx_rpc.py log
     electrumx_rpc.py disconnect

   The first will toggle logging of the sessions.  A logged sesssion
   prints every incoming request to the logs.
   The second will disconnect the sessions.
   Example:  "electrumx_rpc.py log 15 369"

version 0.7.20
--------------

- fix for errors during batch requests (issue #54)
- don't log errors on shutdown after giving sockets time to close

version 0.7.19
--------------

- revert mempool change of 0.7.18

version 0.7.18
--------------

- better IRC support for tor (valesi)
- issues: suppressed some uninteresting socket logging to fix #52
- mempool: fixed small memory leak

version 0.7.17
--------------

- upped read buffer limit to 1,000,000 bytes.

version 0.7.16
--------------

- fix bug introduced in 0.7.12 that hit during reorgs

version 0.7.15
--------------

The following meta variables in your banner file are now replaced in
addition to $VERSION described in the notes to 0.7.11.  If you type
getnetworkinfo in your daemon's debug console you will see what they
are based on:

- $DAEMON_VERSION is replaced with the daemon's version as a dot-separated
  string.  For example "0.12.1".
- $DAEMON_SUBVERSION is replaced with the daemon's user agent string.
  For example, "/BitcoinUnlimited:0.12.1(EB16; AD4)/".

version 0.7.14
--------------

Improved DoS protection:

- incoming network request buffers - which hold incomplete requests
  are limited to 150,000 bytes, which I believe is large for genuine
  clients.  I don't foresee a need to change this so it is hard-coded.
  If an incoming request (for example, text without a newline) exceeds
  this limit the connection is dropped and the event logged.
- RPC connections have high MAX_SEND and incoming buffer limits as these
  connections are assumed to be trusted.
- new environment variable BANDWIDTH_LIMIT.  See docs/ENV-NOTES.
- fixes: LOG_SESSIONS of 0.7.13 wasn't being properly interpreted.
  Tweak to rocksdb close() that should permit db reopening to work.

version 0.7.13
--------------

- the output of the RPC sessions and getinfo calls are now written to logs
  periodically by default.  See LOG_SESSIONS in docs/ENV-NOTES
- Litecoin update (santzi)

version 0.7.12
--------------

- minor bug fixes: 2 in JSON RPC, 1 in get_utxos (affected addresslistunspent)
- leveldb / rocksdb are opened with a different maximum open files limit,
  depending on whether the chain has been fully synced or not.  If synced
  you want the files for network sockets, if not synced for the DB engines.
  Once synced the DB will be reopened with the lower limit to free up the
  files for serving network connections
- various refactoring preparing for possible asynchronous block processing

version 0.7.11
--------------

- increased MAX_SEND default value to 1 million bytes so as to be able
  to serve large historical transactions of up to ~500K in size.  The
  MAX_SEND floor remains at 350,000 bytes so you can reduce it if you
  wish.  To serve any historical transaction for bitcoin youd should
  set this to around 2,000,100 bytes (one byte becomes 2 ASCII hex chars)
- issue #46: fix reorgs for coinbase-only blocks.  We would not distinguish
  undo information being empty from it not existing
- issue #47: IRC reconnects.  I don't get this issue so cannot be certain
  it is resolved
- $VERSION in your banner file is now replaced with your ElectrumX version
- more work to isolate the DB from the block processor, working towards the
  goal of asynchronous block updates

version 0.7.10
--------------

- replaced MAX_HIST environment variable with MAX_SEND, see docs/ENV-NOTES.
  Large requests are blocked and logged.  The logs should help you determine
  if the requests are genuine (perhaps requiring a higher MAX_SEND) or abuse.

version 0.7.9
-------------

- rewrite jsonrpc.py to also operate as a client.  Use this class
  for a robust electrumx_rpc.py.  Fixes issue #43

version 0.7.8
-------------

- hopefully fix failed assertion on reorgs, issue #44

version 0.7.7
-------------

- add MAX_HIST to throttle history requests; see docs/ENV-NOTES.  One
  provider of ElectrumX services was attacked by a loser requesting
  long histories; this environment variable allows you to limit what
  you attempt to serve.

version 0.7.6
-------------

- Fix IRC regression of 0.7.5 - would always connect to IRC by default

version 0.7.5
-------------

- refactoring of server manager and event handling.  One side effect
  is to fix a bug in 0.7.4 where after a reorg ElectrumX might create
  a second mempool and/or kick off more servers.  Your testing would
  be appreciated.  This is part of the refactoring necessary to
  process incoming blocks asynchronously so client connections are not
  left waiting for several seconds
- close connections on first bad JSON encoding.  Do not process buffered
  requests of a closing connection
- make IRC params a function of the coin (TheLazier) and supply them for
  Dash

version 0.7.4
-------------

- really fix reorgs, they still triggered an assertion.  If you hit a reorg
  I believe your DB is fine and all you need to do is restart with updated
  software
- introduced a new debug env var FORCE_REORG which I used to simulate a
  reorg and confirm they should work

version 0.7.3
-------------

- fix reorgs - broken since 0.6 I think

version 0.7.2
-------------

- don't log message decoding errors.  Cut off a connection after it has sent
  10 ill-formed requests.
- doc improvements (cluelessperson)
- RPC ports for Dash (TheLazier)

version 0.7.1
-------------

- fixes an unqualified use of RPCError

version 0.7
-----------

- daemon failover is now supported; see docs/ENV-NOTES.  As a result,
  DAEMON_URL must now be supplied and DAEMON_USERNAME, DAEMON_PASSWORD,
  DAEMON_HOST and DAEMON_PORT are no longer used.
- fixed a bug introduced in 0.6 series where some client header requests
  would fail
- fully asynchronous mempool handling; blocks can be processed and clients
  notified whilst the mempool is still being processed

version 0.6.3
-------------

- new environment variables MAX_SUBS and MAX_SESSION_SUBS.  Please read
  docs/ENV-NOTES - I encourage you to raise the default values.
- fixed import bug in 0.6.2 that prevented initial sync
- issues closed: #30.  Logs should be clean on shutdown now.

version 0.6.2
-------------

- handle daemon errors properly that result from client requests; pass the
  error onto the client
- start serving immediatley on catchup; don't wait for the mempool
- logging improvements, in particular logging software and DB versions
- issues closed: #29, #31, #32

version 0.6.1
-------------

- main focus was better logging - more concise and informative, particularly
  when caught up
- issues closed: #26, #27
- default reorg limit is now taken from the coin, with a high default for
  bitcoin testnet

version 0.6.0
-------------

- DB format has changed again.  This doesn't give a performance gain
  or reduction that I could measure, but is cleaner in that each table
  entry is now a singleton and not an array, which I much prefer as a
  cleaner solution.  It may enable other goodness in the future.
- Logging is much less noisy when serving clients.  In fact anything
  in your logs that isn't just status updates probably is a bug that I
  would like to know about.  Unfortunately clean shutdown whilst
  serving clients leads to massive log spew.  This is harmless and I
  believe because of my noob status with asyncio.  I intend to fix
  this in a nearby release.
- expensive client requests are intended to yield to other requests
  sufficiently frequently that there should be no noticeable delays or
  pauses under normal load from hog clients.
- Notifications to hog clients are now queued in sequence with their
  request responses.  They used to be sent immediately regardless of
  pending requests which seems less than ideal.
- some trivial improvements and fixes to local RPC query output

version 0.5.1
-------------

- 0.5 changed some cache defaults, only partially intentionally.  For
  some users, including me, the result was a regression (a 15hr HDD
  sync became a 20hr sync).  Another user reported their fastest sync
  yet (sub 10hr SSD sync).  What changed was memory accounting - all
  releases until 0.5 were not properly accounting for memory usage of
  unflushed transaction hashes.  In 0.5 they were accounted for in the
  UTXO cache, which resulted in much earlier flushes.  0.5.1 flushes
  the hashes at the same time as history so I now account for it
  towards the history cache limit.  To get a reasonable comparison
  with prior releases your HIST_MB environment variable should be
  bumped by about 15% from 0.4 and earlier values.  This will not
  result in greater memory consumption - the additional memory
  consumption was being ignored before but is now being included.
- 0.5.1 is the first release where Electrum client requests are queued
  on a per-session basis.  Previously they were in a global queue.
  This is the beginning of ensuring that expensive / DOS requests
  mostly affect that user's session and not those of other users.  The
  goal is that each session's requests run asynchronously parallel to
  every other sessions's requests.  The missing part of the puzzle is
  that Python's asyncio is co-operative, however at the moment
  ElectrumX does not yield during expensive requests.  I intend that a
  near upcoming release will ensure expensive requests yield the CPU
  at regular fine-grained intervals.  The intended result is that, to
  some extent, expensive requests mainly delay that and later requests
  from the same session, and have minimal impact on the legitimate
  requests of other sessions.  The extent to which this goal is
  achieved will only be verifiable in practice.
- more robust tracking and handling of asynchronous tasks.  I hope
  this will reduce asyncio's logging messages, some of which I'm
  becoming increasingly convinced I have no control over.  In
  particular I learned earlier releases were unintentionally limiting
  the universe of acceptable SSL protocols, and so I made them the
  default that had been intended.
- I added logging of expensive tasks, though I don't expect much real
  information from this
- various RPC improvements

version 0.5
-----------

- DB change: all UTXOs, including those that are not canonically paying to
  an address, are stored in the DB.  So an attempt to spend a UTXO not in
  the DB means corruption.  DB version bumped to 2; older versions will not
  work
- fixed issue #17: the genesis coinbase is not in the UTXO set

version 0.4.3
-------------

- fix exception introduced in 0.4.2

version 0.4.2
-------------

- split out JSON RPC protcol handling.  Now more robust and we should
  fully support JSON RPC 2.0 clients, including batch requests
  (Electrum client does not yet support these)
- refactored and cleaned up server handling
- improved DASH support (thelazier)

version 0.4.1
-------------

- tweak IRC version reporting so we appear in the Electrum client's
  network dialog box

version 0.4
-----------

- IRC connectivity.  See the notes for environment variables, etc.
- logging improvements

Version 0.3.2, 0.3.3
--------------------

- fixed silly bugs

Version 0.3.1
-------------

- fixes issue #9
- save DB version in DB; warn on DB open if incompatible format

Version 0.3
-----------

- Database format has changed; old DBs are incompatible.  They will
  not work and will probably die miserably as I'm not yet versioning
  them for helpful warnings (coming soon).
- The change in on-disk format makes UTXO flushes noticeably more
  efficient.  My gut feeling is it probably benefits HDDs more than
  SSDs, but I have no numbers to back that up other than that my HDD
  synced about 90 minutes (10%) faster.  Until the treacle hits at
  blocks 300k+ there will probably be little noticeable difference in
  sync time.

Version 0.2.3
-------------

- fixes issues #6, #11, #15
- the UTXO cache is now merged with BlockProcessor, where it properly belongs.
  cache.py no longer exists

Version 0.2.2.1
---------------

- fixes issues #12, #13
- only attempt to flush on asyncio.CancelledError to avoid spurious
  secondary errors

Version 0.2.2
-------------

- mostly refactoring: controller.py is gone; cache.py is half-gone.
  Split BlockProcessor into 3: DB, BlockProcessor and BlockServer.  DB
  handles stored DB and FS state; BlockProcessor handles pushing the
  chain forward and caching of updates, and BlockServer will
  additionally serve clients on catchup.  More to come.
- mempool: better logging; also yields during initial seeding
- issues fixed: #10

Version 0.2.1
-------------

- fix rocksdb and lmdb abstractions (bauerj)
- limit concurrent daemon requests
- improve script + coin abstractions
- faster tx and script parsing
- minor bug fixes

Version 0.2
-----------

- update sample run script, remove empty addresses from mempool

Version 0.1
------------

- added setup.py, experimental.  Because of this server_main.py renamed
  electrumx_server.py, and SERVER_MAIN environment variable was renamed
  to ELECTRUMX.  The sample run script was updated to match.
- improvements to logging of daemon connection issues
- removal of old reorg test code
- hopefully more accurate sync ETA

Version 0.07
------------

- fixed a bug introduced in 0.06 at the last minute

Version 0.06
------------

- mempool support.  ElectrumX maintains a representation of the daemon's
  mempool and serves unconfirmed transactions and balances to clients.

Version 0.05
------------

- fixed a bug in 0.04 that stopped ElectrumX serving once synced

Version 0.04
------------

- made the DB interface a little faster for LevelDB and RocksDB; this was
  a small regression in 0.03
- fixed a bug that prevented block reorgs from working
- implement and enable client connectivity.  This is not yet ready for
  public use for several reasons.  Local RPC, and remote TCP and SSL
  connections are all supported in the same way as Electrum-server.
  ElectrumX does not begin listening for incoming connections until it
  has caught up with the daemon's height.  Which ports it is listening
  on will appear in the logs when it starts listening.  The complete
  Electrum wire protocol is implemented, so it is possible to now use
  as a server for your own Electrum client.  Note that mempools are
  not yet handled so unconfirmed transactions will not be notified or
  appear; they will appear once they get in a block.  Also no
  responses are cached, so performance would likely degrade if used by
  many clients.  I welcome feedback on your experience using this.


Version 0.03
------------

- merged bauerj's abstracted DB engine contribution to make it easy to
  play with different backends.  In addition to LevelDB this adds
  support for RocksDB and LMDB.  We're interested in your comparitive
  performance experiences.


Version 0.02
------------

- fix bug where tx counts were incorrectly saved
- large clean-up and refactoring of code, breakout into new files
- several efficiency improvements
- initial implementation of chain reorg handling
- work on RPC and TCP server functionality.  Code committed but not
  functional, so currently disabled
- note that some of the enivronment variables have been renamed,
  see samples/scripts/NOTES for the list